"""Evaluation script for VWW model using manifest-based test splits.

This script evaluates a trained model on test_public or test_hidden splits
generated by create_main_datasplit.py.

IMPORTANT: Test sets (test_public, test_hidden) are write-protected.
DO NOT use test data for training, validation, or hyperparameter tuning.
"""

import os
import argparse
import numpy as np
import time
import math
import json
import resource
from PIL import Image
import tensorflow as tf

IMAGE_SIZE = 96
BASE_DIR = os.path.join(os.getcwd(), 'vw_coco2014_96')
SPLITS_DIR = os.path.join(os.getcwd(), 'splits')

# Evaluation settings
WARMUP_IMAGES = 50
NUM_THREADS = 1  # Fixed thread count for fair comparison


def load_manifest(manifest_path):
    """Load image paths from manifest file."""
    with open(manifest_path, 'r') as f:
        return [line.strip() for line in f if line.strip()]


def load_and_preprocess_image(image_path, use_tflite=False):
    """Load an image and preprocess it for model input."""
    img = Image.open(image_path).convert('RGB')
    img = img.resize((IMAGE_SIZE, IMAGE_SIZE))
    img_array = np.array(img, dtype=np.float32) / 255.0
    
    if use_tflite:
        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    
    return img_array


def get_exact_macs(model_path):
    """Calculate exact MACs from TFLite model structure."""
    if not model_path or not os.path.exists(model_path):
        return None
        
    try:
        import tflite
        
        with open(model_path, 'rb') as f:
            buf = f.read()
            model = tflite.Model.GetRootAsModel(buf, 0)

        graph = model.Subgraphs(0)
        total_flops = 0.0

        for i in range(graph.OperatorsLength()):
            op = graph.Operators(i)
            op_code = model.OperatorCodes(op.OpcodeIndex())
            builtin_code = op_code.BuiltinCode()

            op_flops = 0.0

            # 1. Standard Convolution
            if builtin_code == tflite.BuiltinOperator.CONV_2D:
                out_shape = graph.Tensors(op.Outputs(0)).ShapeAsNumpy()
                filter_shape = graph.Tensors(op.Inputs(1)).ShapeAsNumpy()
                op_flops = 2 * out_shape[1] * out_shape[2] * filter_shape[0] * filter_shape[1] * filter_shape[2] * filter_shape[3]

            # 2. Depthwise Convolution
            elif builtin_code == tflite.BuiltinOperator.DEPTHWISE_CONV_2D:
                out_shape = graph.Tensors(op.Outputs(0)).ShapeAsNumpy()
                filter_shape = graph.Tensors(op.Inputs(1)).ShapeAsNumpy()
                op_flops = 2 * out_shape[1] * out_shape[2] * filter_shape[0] * filter_shape[1] * filter_shape[2] * filter_shape[3]

            # 3. Fully Connected (Dense)
            elif builtin_code == tflite.BuiltinOperator.FULLY_CONNECTED:
                filter_shape = graph.Tensors(op.Inputs(1)).ShapeAsNumpy()
                op_flops = 2 * filter_shape[0] * filter_shape[1]

            total_flops += op_flops

        # Convert to MegaMACs (1 MAC = 2 FLOPs)
        mega_macs = (total_flops / 2) / 1_000_000
        
        print(f"[INFO] Analyzed TFLite: {total_flops/1e6:.2f} MFLOPs -> {mega_macs:.4f} MegaMACs")
        return mega_macs

    except Exception as e:
        print(f"[WARN] Failed to parse TFLite structure: {e}")
        return None


def calculate_score(accuracy, model_size_mb, macs_m):
    """Calculate competition score with size and MACs penalties."""
    if model_size_mb < 0.0001:
        model_size_mb = 0.0001
    size_penalty = 0.3 * math.log10(model_size_mb)
    macs_penalty = 0.001 * macs_m
    return accuracy - size_penalty - macs_penalty


def evaluate_keras_model(model_path, manifest_path):
    """Evaluate a Keras (.h5) model on manifest-based test set."""
    print(f"Loading Keras model from: {model_path}")
    model = tf.keras.models.load_model(model_path)
    
    image_paths = load_manifest(manifest_path)
    print(f"Loaded {len(image_paths)} images from manifest")
    
    correct = 0
    total = 0
    
    for rel_path in image_paths:
        full_path = os.path.join(BASE_DIR, rel_path)
        true_label = 1 if rel_path.startswith('person/') else 0
        
        img = load_and_preprocess_image(full_path, use_tflite=False)
        img_batch = np.expand_dims(img, axis=0)
        
        predictions = model.predict(img_batch, verbose=0)
        pred_label = np.argmax(predictions[0])
        
        if pred_label == true_label:
            correct += 1
        total += 1
        
        if total % 100 == 0:
            print(f"Processed {total}/{len(image_paths)} images...", end='\r')
    
    accuracy = correct / total if total > 0 else 0
    print(f"\nEvaluation complete: {correct}/{total} correct")
    return accuracy


def evaluate_tflite_model(model_path, manifest_path, measure_latency=True):
    """Evaluate a TFLite model on manifest-based test set with detailed metrics."""
    print(f"Loading TFLite model from: {model_path}")
    
    # Set thread count for fair comparison
    os.environ['TFLITE_NUM_THREADS'] = str(NUM_THREADS)
    
    interpreter = tf.lite.Interpreter(
        model_path=model_path,
        num_threads=NUM_THREADS
    )
    interpreter.allocate_tensors()
    
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    
    image_paths = load_manifest(manifest_path)
    print(f"Loaded {len(image_paths)} images from manifest")
    print(f"Thread count: {NUM_THREADS}")
    
    correct = 0
    total = 0
    latencies = []
    
    # Single evaluation loop: accuracy on ALL images, latency on post-warmup only
    warmup_count = WARMUP_IMAGES if measure_latency else 0
    if warmup_count > len(image_paths):
        warmup_count = 0
    
    if warmup_count > 0:
        print(f"\n[EVALUATION] Starting evaluation (first {warmup_count} images are warmup)...")
    else:
        print("\n[EVALUATION] Starting evaluation...")
    
    for idx, rel_path in enumerate(image_paths):
        full_path = os.path.join(BASE_DIR, rel_path)
        true_label = 1 if rel_path.startswith('person/') else 0
        
        # Time full pipeline for post-warmup images only
        if measure_latency and idx >= warmup_count:
            start_time = time.perf_counter()
            img = load_and_preprocess_image(full_path, use_tflite=True)
            interpreter.set_tensor(input_details[0]['index'], img)
            interpreter.invoke()
            output_data = interpreter.get_tensor(output_details[0]['index'])
            pred_label = np.argmax(output_data[0])
            end_time = time.perf_counter()
            latencies.append((end_time - start_time) * 1000)  # Convert to ms
        else:
            # Warmup images or no latency measurement: just evaluate accuracy
            img = load_and_preprocess_image(full_path, use_tflite=True)
            interpreter.set_tensor(input_details[0]['index'], img)
            interpreter.invoke()
            output_data = interpreter.get_tensor(output_details[0]['index'])
            pred_label = np.argmax(output_data[0])
        
        if pred_label == true_label:
            correct += 1
        total += 1
        
        if total % 100 == 0:
            print(f"Processed {total}/{len(image_paths)} images...", end='\r')
    
    accuracy = correct / total if total > 0 else 0
    print(f"\nEvaluation complete: {correct}/{total} correct")
    
    # Calculate latency percentiles
    latency_stats = {}
    if measure_latency and latencies:
        latencies_sorted = sorted(latencies)
        latency_stats = {
            'mean': np.mean(latencies),
            'p50': np.percentile(latencies, 50),
            'p90': np.percentile(latencies, 90),
            'p99': np.percentile(latencies, 99),
            'min': np.min(latencies),
            'max': np.max(latencies)
        }
    
    return accuracy, latency_stats


def main():
    parser = argparse.ArgumentParser(
        description="Evaluate VWW model on manifest-based test splits",
        epilog="NOTE: Test sets are write-protected. Never train on test data!"
    )
    parser.add_argument("--model", type=str, required=True, 
                        help="Path to model file (.h5 or .tflite)")
    parser.add_argument("--split", type=str, default="test_public",
                        choices=["test_public", "test_hidden", "val"],
                        help="Which split to evaluate on")
    parser.add_argument("--data_dir", type=str, default=None,
                        help="Override BASE_DIR (default: vw_coco2014_96)")
    parser.add_argument("--splits_dir", type=str, default=None,
                        help="Override SPLITS_DIR (default: splits)")
    parser.add_argument("--no_latency", action="store_true",
                        help="Skip latency measurements (faster evaluation)")
    parser.add_argument("--compute_score", action="store_true",
                        help="Compute competition score (requires TFLite model)")
    parser.add_argument("--export_json", action="store_true",
                        help="Export metadata JSON for Pi deployment (if accuracy >= 80%%)")
    parser.add_argument("--threads", type=int, default=NUM_THREADS,
                        help=f"Number of threads for TFLite (default: {NUM_THREADS})")
    args = parser.parse_args()
    
    # Override directories if provided
    global BASE_DIR, SPLITS_DIR, NUM_THREADS
    if args.data_dir:
        BASE_DIR = args.data_dir
    if args.splits_dir:
        SPLITS_DIR = args.splits_dir
    NUM_THREADS = args.threads
    
    # Validate test set usage
    if args.split in ["test_public", "test_hidden"]:
        print("\n" + "="*60)
        print("⚠️  WARNING: EVALUATING ON TEST SET")
        print("="*60)
        print("Test sets are WRITE-PROTECTED and must NOT be used for:")
        print("  - Training")
        print("  - Validation")
        print("  - Hyperparameter tuning")
        print("  - Model selection")
        print("Only use test sets for final evaluation of your chosen model.")
        print("="*60 + "\n")
    
    manifest_path = os.path.join(SPLITS_DIR, f"{args.split}.txt")
    
    if not os.path.exists(manifest_path):
        print(f"Error: Manifest file not found: {manifest_path}")
        print("Please run create_main_datasplit.py first to generate splits.")
        return
    
    if not os.path.exists(args.model):
        print(f"Error: Model file not found: {args.model}")
        return
    
    # Check CPU governor (informational only)
    try:
        with open('/sys/devices/system/cpu/cpu0/cpufreq/scaling_governor', 'r') as f:
            governor = f.read().strip()
            if governor != 'performance':
                print(f"[INFO] CPU governor: {governor}")
                print("[RECOMMEND] Set to 'performance' for consistent latency measurements")
                print("            sudo cpupower frequency-set -g performance\n")
            else:
                print(f"[INFO] CPU governor: {governor} ✓\n")
    except:
        print("[INFO] Could not read CPU governor (may not be on Linux)\n")
    
    # Determine model type and evaluate
    measure_latency = not args.no_latency
    
    if args.model.endswith('.tflite'):
        accuracy, latency_stats = evaluate_tflite_model(
            args.model, manifest_path, measure_latency
        )
        
        # Get model size and MACs
        size_mb = os.path.getsize(args.model) / (1024 * 1024)
        macs_m = get_exact_macs(args.model)
        macs_source = "TFLite model analysis" if macs_m is not None else None
        
        # Calculate score if requested
        score = None
        if args.compute_score:
            if macs_m is None:
                print("\n" + "="*60)
                print("[ERROR] Cannot compute score: MACs computation failed")
                print("="*60)
                print("Failed to extract MACs from TFLite model.")
                print("This may happen if:")
                print("  - The model uses unsupported operations")
                print("  - The tflite library is not installed")
                print()
                print("Try installing: pip install tflite")
                print("="*60)
                return 1  # Exit with error code
            score = calculate_score(accuracy, size_mb, macs_m)
        
        # Get peak memory
        peak_memory_mb = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024
        
    elif args.model.endswith('.h5'):
        accuracy = evaluate_keras_model(args.model, manifest_path)
        latency_stats = {}
        size_mb = os.path.getsize(args.model) / (1024 * 1024)
        macs_m = None
        macs_source = None
        score = None
        peak_memory_mb = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024
        
        if args.compute_score:
            print("\n" + "="*60)
            print("[ERROR] Cannot compute score with .h5 model")
            print("="*60)
            print("Score computation requires TFLite model for MACs calculation.")
            print("Convert your model to TFLite first:")
            print()
            print("  converter = tf.lite.TFLiteConverter.from_keras_model(model)")
            print("  tflite_model = converter.convert()")
            print("  with open('model.tflite', 'wb') as f:")
            print("      f.write(tflite_model)")
            print("="*60)
            return 1  # Exit with error code
    else:
        print("Error: Model must be .h5 (Keras) or .tflite format")
        return 1
    
    # Print results
    print("\n" + "="*60)
    print(f"EVALUATION RESULTS")
    print("="*60)
    print(f"Model:        {os.path.basename(args.model)}")
    print(f"Split:        {args.split}")
    print(f"Accuracy:     {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"Model Size:   {size_mb:.4f} MB")
    
    if macs_m is not None:
        print(f"MACs:         {macs_m:.4f} M (from {macs_source})")
    
    if latency_stats:
        print("-" * 60)
        # Highlight official latency metric
        p90_latency = latency_stats['p90']
        print(f"LATENCY (OFFICIAL): {p90_latency:.2f} ms (p90)")
        print()
        print("Detailed Latency Statistics (ms):")
        print(f"  Mean:       {latency_stats['mean']:.2f}")
        print(f"  p50:        {latency_stats['p50']:.2f}")
        print(f"  p90:        {latency_stats['p90']:.2f}")
        print(f"  p99:        {latency_stats['p99']:.2f}")
        print(f"  Min:        {latency_stats['min']:.2f}")
        print(f"  Max:        {latency_stats['max']:.2f}")
        print(f"  Threads:    {NUM_THREADS}")
    
    print(f"Peak Memory:  {peak_memory_mb:.2f} MB")
    
    if score is not None:
        print("-" * 60)
        print(f"SCORE:        {score:.4f}")
    
    print("="*60)
    
    # Export JSON for Pi if requested
    if args.export_json and args.model.endswith('.tflite'):
        if accuracy >= 0.80:
            json_path = args.model.replace(".tflite", ".json")
            metadata = {
                "model_name": os.path.basename(args.model),
                "split": args.split,
                "accuracy": accuracy,
                "size_mb": size_mb,
                "macs_m": macs_m,
                "latency_p50_ms": latency_stats.get('p50', None),
                "latency_p90_ms": latency_stats.get('p90', None),
                "latency_p99_ms": latency_stats.get('p99', None),
                "threads": NUM_THREADS,
                "created_at": time.ctime()
            }
            
            if score is not None:
                metadata["score"] = score
            
            with open(json_path, "w") as f:
                json.dump(metadata, f, indent=4)
            
            print(f"\n[SUCCESS] Metadata exported to: {json_path}")
            print(f"          Transfer BOTH .tflite and .json to Pi for deployment.")
        else:
            print(f"\n[FAIL] Accuracy < 80%. Metadata JSON NOT generated.")


if __name__ == "__main__":
    main()
